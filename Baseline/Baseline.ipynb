{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HdMyi2D4-6YC"
   },
   "source": [
    "# **Model 2 seq2seq for text Summurization using seq2seq lib**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IzJv9UUti_og"
   },
   "source": [
    "### Intro\n",
    "This is a modification to https://github.com/dongjun-Lee/text-summarization-tensorflow \n",
    "I am builging it in a notebook envronment to be able to easily integrate with colab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6lSefB0pn9Gq"
   },
   "source": [
    "## Helpers (Googel Drive , Utilits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2pNpy7UgCx9m"
   },
   "source": [
    "### Utilits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X0VWHFAnoDL4"
   },
   "source": [
    "https://github.com/dongjun-Lee/text-summarization-tensorflow/blob/master/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "colab_type": "code",
    "id": "hf4b16KopBx7",
    "outputId": "357927ef-5859-40c2-b8a6-75a9e25b977c"
   },
   "outputs": [],
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H1tUe8ILummG"
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/38088652/pandas-convert-categories-to-numbers\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "#ex = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'\n",
    "\n",
    "def get_pos_tags_dict(words):\n",
    "    #sent = nltk.word_tokenize(sent)\n",
    "    #print(sent)\n",
    "    post_tags_for_words = nltk.pos_tag(words)\n",
    "\n",
    "    pos_list ={}\n",
    "    #sent = preprocess(ex)\n",
    "    for word,pos in post_tags_for_words:\n",
    "        pos_list[word] = pos\n",
    "    #print(pos_list)\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(list(pos_list.items()))\n",
    "    df.columns = ['word', 'pos']\n",
    "    df.pos = pd.Categorical(df.pos)\n",
    "    df['code'] = df.pos.cat.codes\n",
    "    #print(df)\n",
    "\n",
    "    pos_list ={}\n",
    "    for index, row in df.iterrows():\n",
    "        pos_list[row['word']] = row['code']\n",
    "#     print(pos_list)\n",
    "    return pos_list , post_tags_for_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lA1tCj2mn_2x"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import collections\n",
    "import pickle\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# default_path = \"drive/My Drive/NLP/project/rnn-lstm/data/\"\n",
    "default_path = \"/media/mount/Users/Ruchit Modi/Documents/CSE538/project/Model2/dataset/\"\n",
    "\n",
    "train_article_path = default_path + \"sumdata/train/train.article.txt\"\n",
    "train_title_path   = default_path + \"sumdata/train/train.title.txt\"\n",
    "valid_article_path = default_path + \"sumdata/train/valid.article.filter.txt\"\n",
    "valid_title_path   = default_path + \"sumdata/train/valid.title.filter.txt\"\n",
    "\n",
    "textrank_train_len = 200000\n",
    "textrank_valid_len = 50\n",
    "\n",
    "#valid_article_path = default_path + \"sumdata/DUC2003/input.txt\"\n",
    "#valid_title_path   = default_path + \"sumdata/DUC2003/task1_ref0.txt\"\n",
    "\n",
    "def clean_str(sentence):\n",
    "    sentence = re.sub(\"[#.]+\", \"#\", sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def get_text_list(data_path, toy):\n",
    "    with open (data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        if not toy:\n",
    "            return [clean_str(x.strip()) for x in f.readlines()][:textrank_train_len]\n",
    "        else:\n",
    "            return [clean_str(x.strip()) for x in f.readlines()][:textrank_valid_len]\n",
    "        \n",
    "def build_dict(step, toy=False):\n",
    "    if step == \"train\":\n",
    "        train_article_list = get_text_list(train_article_path, toy)\n",
    "        train_title_list = get_text_list(train_title_path, toy)\n",
    "\n",
    "        words = list()\n",
    "        for sentence in train_article_list + train_title_list:\n",
    "            for word in word_tokenize(sentence):\n",
    "                words.append(word)\n",
    "\n",
    "        word_counter = collections.Counter(words).most_common()\n",
    "        word_dict = dict()\n",
    "        word_dict[\"<padding>\"] = 0\n",
    "        word_dict[\"<unk>\"] = 1\n",
    "        word_dict[\"<s>\"] = 2\n",
    "        word_dict[\"</s>\"] = 3\n",
    "        for word, _ in word_counter:\n",
    "            word_dict[word] = len(word_dict)\n",
    "\n",
    "        with open(default_path + \"word_dict.pickle\", \"wb\") as f:\n",
    "            pickle.dump(word_dict, f)\n",
    "\n",
    "    elif step == \"valid\":\n",
    "        with open(default_path + \"word_dict.pickle\", \"rb\") as f:\n",
    "            word_dict = pickle.load(f)\n",
    "\n",
    "    reversed_dict = dict(zip(word_dict.values(), word_dict.keys()))\n",
    "\n",
    "    article_max_len = 50\n",
    "    summary_max_len = 15\n",
    "\n",
    "    return word_dict, reversed_dict, article_max_len, summary_max_len\n",
    "\n",
    "\n",
    "def build_dataset(step, word_dict, article_max_len, summary_max_len, toy=False):\n",
    "    if step == \"train\":\n",
    "        article_list = get_text_list(train_article_path, toy)\n",
    "        title_list = get_text_list(train_title_path, toy)\n",
    "    elif step == \"valid\":\n",
    "        # Only when args.toy == True\n",
    "        article_list = get_text_list(valid_article_path, toy)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    x = [word_tokenize(d) for d in article_list]\n",
    "    x = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in d] for d in x]\n",
    "    x = [d[:article_max_len] for d in x]\n",
    "    x = [d + (article_max_len - len(d)) * [word_dict[\"<padding>\"]] for d in x]\n",
    "    \n",
    "    if step == \"valid\":\n",
    "        return x\n",
    "    else:        \n",
    "        y = [word_tokenize(d) for d in title_list]\n",
    "        y = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in d] for d in y]\n",
    "        y = [d[:(summary_max_len - 1)] for d in y]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def batch_iter(inputs, outputs, batch_size, num_epochs):\n",
    "    inputs = np.array(inputs)\n",
    "    outputs = np.array(outputs)\n",
    "\n",
    "    sentence_ids = [[i] for i in range(inputs.shape[0])]\n",
    "    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, len(inputs))\n",
    "            yield inputs[start_index:end_index], outputs[start_index:end_index], sentence_ids[start_index:end_index]\n",
    "\n",
    "\n",
    "def get_init_embedding(word_dict , reversed_dict, embedding_size):\n",
    "    print(\"Loading Lists...\")\n",
    "    train_article_list = get_text_list(train_article_path, False)\n",
    "    train_title_list = get_text_list(train_title_path, False)\n",
    "\n",
    "    print(\"Loading TF-IDF...\")\n",
    "    tf_idf_list = tf_idf_generate(train_article_list+train_title_list)\n",
    "    \n",
    "    print(\"Loading Pos Tags...\")\n",
    "    pos_list , postags_for_named_entity = get_pos_tags_dict(word_dict.keys())\n",
    "\n",
    "    #print(\"Loading Named Entity...\")\n",
    "    #named_entity_recs = named_entity(postags_for_named_entity) \n",
    "    \n",
    "    print(\"Loading Glove vectors...\")\n",
    "\n",
    "    with open( default_path + \"glove/model_glove_300.pkl\", 'rb') as handle:\n",
    "        word_vectors = pickle.load(handle)     \n",
    "    \n",
    "    used_words = 0\n",
    "    word_vec_list = list()\n",
    "    for _, word in sorted(reversed_dict.items()):\n",
    "        try:\n",
    "            word_vec = word_vectors.word_vec(word)\n",
    "            if word in tf_idf_list:\n",
    "                v= tf_idf_list[word]\n",
    "                rich_feature_array = np.array([v,v,v,v,v,v,v,v,v,v])\n",
    "                word_vec = np.append(word_vec, rich_feature_array)\n",
    "            else:\n",
    "                v=0\n",
    "                rich_feature_array = np.array([v,v,v,v,v,v,v,v,v,v])\n",
    "                word_vec = np.append(word_vec, rich_feature_array)\n",
    "\n",
    "            if word in pos_list:\n",
    "                v=pos_list[word]\n",
    "                rich_feature_array_2 = np.array([v,v,v,v,v,v,v,v,v,v])\n",
    "                word_vec = np.append(word_vec, rich_feature_array_2)\n",
    "            else:\n",
    "                v=0\n",
    "                rich_feature_array_2 = np.array([v,v,v,v,v,v,v,v,v,v])\n",
    "                word_vec = np.append(word_vec, rich_feature_array_2) \n",
    "\n",
    "            #if word in named_entity_recs:\n",
    "            #  v=named_entity_recs[word]\n",
    "            #  rich_feature_array_3 = np.array([v,v,v,v,v,v,v,v,v,v])\n",
    "            #  word_vec = np.append(word_vec, rich_feature_array_3)\n",
    "            #else:\n",
    "            #  v=0\n",
    "            #  rich_feature_array_3 = np.array([v,v,v,v,v,v,v,v,v,v])\n",
    "            #  word_vec = np.append(word_vec, rich_feature_array_3)  \n",
    "          \n",
    "            used_words += 1\n",
    "        except KeyError:\n",
    "            word_vec = np.zeros([embedding_size], dtype=np.float32) #to generate for <padding> and <unk>\n",
    "        \n",
    "        \n",
    "        word_vec_list.append(np.array(word_vec))\n",
    "\n",
    "    print(\"words found in glove percentage = \" + str((used_words/len(word_vec_list))*100) )\n",
    "          \n",
    "    # Assign random vector to <s>, </s> token\n",
    "    word_vec_list[2] = np.random.normal(0, 1, embedding_size)\n",
    "    word_vec_list[3] = np.random.normal(0, 1, embedding_size)\n",
    "\n",
    "    return np.array(word_vec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZoqnXswL_FzR"
   },
   "outputs": [],
   "source": [
    "# _____TF-IDF libraries_____\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# _____helper Libraries_____\n",
    "import pickle  # would be used for saving temp files\n",
    "import csv     # used for accessing the dataset\n",
    "import timeit  # to measure time of training\n",
    "import random  # used to get a random number\n",
    "\n",
    "\n",
    "def tf_idf_generate(sentences):\n",
    "    #https://stackoverflow.com/questions/30976120/find-the-tf-idf-score-of-specific-words-in-documents-using-sklearn\n",
    "\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "    # our corpus\n",
    "    data = sentences\n",
    "\n",
    "    cv = CountVectorizer()\n",
    "\n",
    "    # convert text data into term-frequency matrix\n",
    "    data = cv.fit_transform(data)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "    # convert term-frequency matrix into tf-idf\n",
    "    tfidf_matrix = tfidf_transformer.fit_transform(data)\n",
    "\n",
    "    # create dictionary to find a tfidf word each word\n",
    "    word2tfidf = dict(zip(cv.get_feature_names(), tfidf_transformer.idf_))\n",
    "\n",
    "    #i = 0\n",
    "    #for word, score in word2tfidf.items():\n",
    "    #    print(word, score)\n",
    "    #    if (i == 10):\n",
    "    #      break\n",
    "    #    i+=1  \n",
    "  \n",
    "    return word2tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a0pP9BSA-h10"
   },
   "outputs": [],
   "source": [
    "# https://nlpforhackers.io/named-entity-extraction/\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "\n",
    "# sentence = \"Mark and John are working at Google.\"\n",
    "\n",
    "\n",
    "# print (ne_chunk(pos_tag(word_dict.keys())[:5]))\n",
    "# names = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "\n",
    "# names = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "\n",
    "def named_entity(post_tags_for_words):\n",
    "    names = ne_chunk(post_tags_for_words)\n",
    "    names_dict = {}\n",
    "    for n in names:\n",
    "        if (len(n) == 1):\n",
    "            named_entity = str(n).split(' ')[0][1:]\n",
    "            word = str(n).split(' ')[1].split('/')[0]\n",
    "            names_dict[word] = named_entity\n",
    "    print(names_dict)\n",
    "\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.DataFrame(list(names_dict.items()))\n",
    "    df.columns = ['word', 'pos']\n",
    "    df.pos = pd.Categorical(df.pos)\n",
    "    df['code'] = df.pos.cat.codes\n",
    "    # print(df)\n",
    "\n",
    "    names_dict = {}\n",
    "    for index, row in df.iterrows():\n",
    "        names_dict[row['word']] = row['code']\n",
    "    print(names_dict)\n",
    "    return names_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ShEw2241Ecsn",
    "outputId": "1899e04d-e7e4-4bda-bfa0-640552191d13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n",
      "Loading training dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Building dictionary...\")\n",
    "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"train\", False)\n",
    "print(\"Loading training dataset...\")\n",
    "train_x, train_y = build_dataset(\"train\", word_dict, article_max_len, summary_max_len, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "colab_type": "code",
    "id": "YIbIcmz8N8Nc",
    "outputId": "a489e0eb-4e46-4bba-87ed-ed53e8eae5be"
   },
   "outputs": [],
   "source": [
    "pos_list , postags_for_named_entity = get_pos_tags_dict(word_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "vGAAv_HiOCGK",
    "outputId": "d84ec773-39ab-4503-9c23-1677b8744912"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Lists...\n",
      "Loading TF-IDF...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Lists...\")\n",
    "train_article_list = get_text_list(train_article_path, False)\n",
    "train_title_list = get_text_list(train_title_path, False)\n",
    "\n",
    "print(\"Loading TF-IDF...\")\n",
    "tf_idf_list = tf_idf_generate(train_article_list+train_title_list)\n",
    "# tf_idf_list[\"apple\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "colab_type": "code",
    "id": "B1UVvbwwOs08",
    "outputId": "4c49e99d-8c76-431a-8646-d3cbb25b881c"
   },
   "outputs": [],
   "source": [
    "# pos_list[\"apple\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "4XW03jfHP2pS",
    "outputId": "0f9811d3-6a57-4055-88af-0bf6b3ff8595"
   },
   "outputs": [],
   "source": [
    "# named_entity_recs = named_entity(postags_for_named_entity) \n",
    "# named_entity_recs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZxFTlhgDsfUF"
   },
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0dZvXIVXsiEt"
   },
   "source": [
    "### POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "c5YAAeXrsuwe",
    "outputId": "d98036d2-d668-4d7c-f0fe-0ff76eddb295"
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tKBuNALo-idI"
   },
   "source": [
    "### Named Entity Reognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "svj4Nx7X-wSn",
    "outputId": "a4f0ed6d-20b9-4b71-b66d-a3962222d7ea"
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qI3AJND0mb0r"
   },
   "source": [
    "## Prepare Data (unzip , discover operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X1B9ZgJqmsni"
   },
   "source": [
    "\n",
    "https://github.com/dongjun-Lee/text-summarization-tensorflow/blob/master/prep_data.py\n",
    "\n",
    "1.   Word Embedding : Used [Glove pre-trained vectors](https://nlp.stanford.edu/projects/glove/ ) to initialize word embedding.  \n",
    "2.   Dataset :  Dataset is available at [harvardnlp/sent-summary](https://github.com/harvardnlp/sent-summary). Locate the summary.tar.gz file in project root directory.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BIf0duZCk3aw"
   },
   "source": [
    "#### unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "g2dpVeOqnJAh",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wget'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-804967440bf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wget'"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "import os\n",
    "import tarfile\n",
    "import gzip\n",
    "import zipfile\n",
    "import argparse\n",
    "\n",
    "\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument(\"--glove\", action=\"store_true\")\n",
    "#args = parser.parse_args()\n",
    "\n",
    "# Extract data file\n",
    "#with tarfile.open(default_path + \"sumdata/train/summary.tar.gz\", \"r:gz\") as tar:\n",
    "#    tar.extractall()\n",
    "\n",
    "# with gzip.open(default_path + \"sumdata/train/train.article.txt.gz\", \"rb\") as gz:\n",
    "#     with open(default_path + \"sumdata/train/train.article.txt\", \"wb\") as out:\n",
    "#         out.write(gz.read())\n",
    "\n",
    "# with gzip.open(default_path + \"sumdata/train/train.title.txt.gz\", \"rb\") as gz:\n",
    "#     with open(default_path + \"sumdata/train/train.title.txt\", \"wb\") as out:\n",
    "#         out.write(gz.read())\n",
    "\n",
    "        \n",
    "#if args.glove:\n",
    "#    glove_dir = \"glove\"\n",
    "#    glove_url = \"https://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip\"\n",
    "#\n",
    "#    if not os.path.exists(glove_dir):\n",
    "#        os.mkdir(glove_dir)\n",
    "#\n",
    "#    # Download glove vector\n",
    "#    wget.download(glove_url, out=glove_dir)\n",
    "#\n",
    "#    # Extract glove file\n",
    "#    with zipfile.ZipFile(os.path.join(\"glove\", \"glove.42B.300d.zip\"), \"r\") as z:\n",
    "#        z.extractall(glove_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "sIpdo8iI0-JN",
    "outputId": "a4a69110-3b4d-48be-c1c5-235bc4d0a426"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Baseline(tf_idf_,_pos_tags).ipynb'\n",
      "'Model_2_features(tf_idf_,_pos_tags).ipynb'\n",
      " Model_4_generator_.ipynb\n",
      " PreProcessTextRank.ipynb\n",
      " TextRank.ipynb\n",
      " TextRank.py\n",
      " Text_Abstraction_v2.ipynb\n",
      "'Text_Summarization_2_features_paper(tf_idf_,_pos_tags).ipynb'\n",
      "'Text_Summarization_2_features_paper(tf_idf_,_pos_tags).py'\n",
      " Untitled.ipynb\n",
      " __pycache__\n",
      " dataset\n",
      " glove\n",
      " glove.42B.300d.zipptbrjjfd.tmp\n",
      " long_stopwords.txt\n",
      " train_article_scores.pickle\n",
      " train_title_scores.pickle\n",
      " valid_article_scores.pickle\n",
      " valid_title_scores.pickle\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cyFi7ETY7NwM"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import wget\n",
    "\n",
    "# glove_dir = \"glove\"\n",
    "# glove_url = \"https://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip\"\n",
    "# #\n",
    "# if not os.path.exists(glove_dir):\n",
    "#     os.mkdir(glove_dir)\n",
    "\n",
    "# # Download glove vector\n",
    "# wget.download(glove_url, out=glove_dir)\n",
    "\n",
    "# # Extract glove file\n",
    "# with zipfile.ZipFile(os.path.join(glove_dir, \"glove.42B.300d.zip\"), \"r\") as z:\n",
    "#     z.extractall(glove_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IvKMw9lAGVif"
   },
   "source": [
    "#### Discover Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nOBeSOwZ_djh"
   },
   "outputs": [],
   "source": [
    "train_article_list = get_text_list(train_article_path, False)\n",
    "train_title_list = get_text_list(train_title_path, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XppjlyoXBZZz"
   },
   "outputs": [],
   "source": [
    "test = tf_idf_generate(train_article_list + train_title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_article_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "R1VhgbIHD0qJ",
    "outputId": "04328190-7b58-441c-9fab-b8f159603305"
   },
   "outputs": [],
   "source": [
    "# test[\"apple\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "RJ-kQgWhFkWP",
    "outputId": "42ff340e-af3b-4dd4-8ce0-dd4da049a1e1"
   },
   "outputs": [],
   "source": [
    "# print(word_dict[\"apple\"])\n",
    "# print(word_dict[\"cat\"])\n",
    "# print(word_dict[\"dog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "8THQucjtF7BW",
    "outputId": "a157c9d4-fffa-40b4-875c-d0cee9b00f0a"
   },
   "outputs": [],
   "source": [
    "# print(reversed_dict[9076])\n",
    "# print(reversed_dict[7243])\n",
    "# print(reversed_dict[4206])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "tg_QrwxVGKeo",
    "outputId": "9af02873-cd31-464c-acfa-b0274027cf98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_max_len : 50\n",
      "summary_max_len : 15\n"
     ]
    }
   ],
   "source": [
    "print(\"article_max_len : \" + str(article_max_len))\n",
    "print(\"summary_max_len : \" + str(summary_max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 904
    },
    "colab_type": "code",
    "id": "Rzg8fisCGZq2",
    "outputId": "65ad2ef0-26f7-4047-a80a-01c446d14d97"
   },
   "outputs": [],
   "source": [
    "# print(train_x[0])\n",
    "# for num in train_x[0] :\n",
    "#     print(reversed_dict[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "yUcUihAGG2ge",
    "outputId": "36798597-cbd2-48d3-9534-16738ff35f2c"
   },
   "outputs": [],
   "source": [
    "# print(train_y[0])\n",
    "# for num in train_y[0] :\n",
    "#   print(reversed_dict[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "n-fGxQPSJ242",
    "outputId": "6be3f927-8f7f-439e-e914-9d893a4eec9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Lists...\n",
      "Loading TF-IDF...\n",
      "Loading Pos Tags...\n",
      "Loading Glove vectors...\n",
      "words found in glove percentage = 91.75771029889796\n"
     ]
    }
   ],
   "source": [
    "test_embedding = get_init_embedding(word_dict , reversed_dict, 320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "r4k6ornzH5XV",
    "outputId": "781dfa13-788b-4f36-b12b-dc13be93bb5b"
   },
   "outputs": [],
   "source": [
    "# #print(reversed_dict[2000])\n",
    "# len(test_embedding[30000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nrpqz0cNzDXT"
   },
   "outputs": [],
   "source": [
    "pos_list = get_pos_tags_dict(word_dict.keys())\n",
    "#pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precompute textrank score sentence wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_article_scores(step, toy=False):\n",
    "    sentence_to_word_dic = {}\n",
    "    cache_miss = 0\n",
    "    total_found = 0\n",
    "    f = open('/media/mount/Users/Ruchit Modi/Documents/CSE538/project/Model2/{}_article_scores.pickle'.format(step), 'rb')\n",
    "    import pickle\n",
    "    article_text_rank_scores = pickle.load(f)\n",
    "    for article_id in article_text_rank_scores:\n",
    "        word_dic = {}\n",
    "        for word, score in article_text_rank_scores[article_id]:\n",
    "            word_id = word_dict.get(word, None)\n",
    "            if word_id is not None:\n",
    "                word_dic[word_id] = score\n",
    "                total_found += 1\n",
    "            else:\n",
    "                cache_miss += 1\n",
    "        sentence_to_word_dic[article_id] = word_dic\n",
    "    print('Cache miss percentage: {} %'.format(100*cache_miss/(cache_miss + total_found)))\n",
    "    return sentence_to_word_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161446512"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(test_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# def get_sentence_text_rank_embeddings(sentence_to_word_dic):\n",
    "#     sentence_textrank_embed = []\n",
    "#     for sentence_id in tqdm(sentence_to_word_dic):\n",
    "#         words_textrank_embed = []\n",
    "#         word_score_dic = sentence_to_word_dic[sentence_id]\n",
    "#         for word_id in word_dict:\n",
    "#             words_textrank_embed.append(word_score_dic.get(word_id, 0.0))\n",
    "#         sentence_textrank_embed.append(np.array(words_textrank_embed))\n",
    "    \n",
    "#     return np.array(sentence_textrank_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sentence_textrank_mat(step, word_dict, article_max_len, summary_max_len, toy=False):\n",
    "    if step == \"train\":\n",
    "        article_list = get_text_list(train_article_path, toy)\n",
    "    elif step == \"valid\":\n",
    "        article_list = get_text_list(valid_article_path, toy)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    sentence_to_word_dic = generate_article_scores(step, toy)\n",
    "\n",
    "    # Building global matrix\n",
    "    \n",
    "\n",
    "    x = [word_tokenize(d) for d in article_list]\n",
    "    sentence_mat_x = []\n",
    "    for idx, sentence in enumerate(x):\n",
    "        word_textrank_dic = sentence_to_word_dic[idx]\n",
    "        word_scores = []\n",
    "        for token in sentence:\n",
    "            word_scores.append(word_textrank_dic.get(word_dict.get(token, None), 0.0))\n",
    "        \n",
    "        word_scores = word_scores[:article_max_len]\n",
    "        word_scores = word_scores + [0.0] * (article_max_len - len(word_scores))\n",
    "        sentence_mat_x.append(np.array(word_scores))\n",
    "   \n",
    "    return np.array(sentence_mat_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache miss percentage: 0.0 %\n"
     ]
    }
   ],
   "source": [
    "sentence_textrank_mat = build_sentence_textrank_mat(\"train\", word_dict, article_max_len, summary_max_len, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 50)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_textrank_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.442070484161377"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(sentence_textrank_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000112"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(sentence_textrank_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F_r1e_f2nCKK"
   },
   "source": [
    "sentence_to_word_dic:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MABp2aWMoNKh"
   },
   "source": [
    "## Encoder-Decoder model with attention mechanism.\n",
    "https://github.com/dongjun-Lee/text-summarization-tensorflow/blob/master/model.py\n",
    "\n",
    "1.   Encoder : Used LSTM cell with stack_bidirectional_dynamic_rnn.\n",
    "2.   Decoder : Used LSTM BasicDecoder for training, and BeamSearchDecoder for inference.\n",
    "3.   Attention Mechanism : Used BahdanauAttention with weight normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GKHwTTZg5eJa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rmodi/anaconda3/envs/nlp-project/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rmodi/anaconda3/envs/nlp-project/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rmodi/anaconda3/envs/nlp-project/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rmodi/anaconda3/envs/nlp-project/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rmodi/anaconda3/envs/nlp-project/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rmodi/anaconda3/envs/nlp-project/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "#from utils import get_init_embedding\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, reversed_dict, article_max_len, summary_max_len, args, forward_only=False):\n",
    "        self.vocabulary_size = len(reversed_dict)\n",
    "        self.embedding_size = args.embedding_size\n",
    "        self.num_hidden = args.num_hidden\n",
    "        self.num_layers = args.num_layers\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.beam_width = args.beam_width\n",
    "        if not forward_only:\n",
    "            self.keep_prob = args.keep_prob\n",
    "        else:\n",
    "            self.keep_prob = 1.0\n",
    "        self.cell = tf.nn.rnn_cell.BasicLSTMCell\n",
    "        with tf.variable_scope(\"decoder/projection\"):\n",
    "            self.projection_layer = tf.layers.Dense(self.vocabulary_size, use_bias=False)\n",
    "\n",
    "        self.batch_size = tf.placeholder(tf.int32, (), name=\"batch_size\")\n",
    "        self.X = tf.placeholder(tf.int32, [None, article_max_len])\n",
    "        self.X_len = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        self.sentence_ids = tf.placeholder(tf.int32, [None, 1])\n",
    "        \n",
    "        self.decoder_input = tf.placeholder(tf.int32, [None, summary_max_len])\n",
    "        self.decoder_len = tf.placeholder(tf.int32, [None])\n",
    "        self.decoder_target = tf.placeholder(tf.int32, [None, summary_max_len])\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        # Debugging variables\n",
    "        self.my_debug_inp = None\n",
    "        self.my_debug_inp2 = None\n",
    "        \n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            sentence_textrank_mat_tf = tf.constant(sentence_textrank_mat, dtype=tf.float32)\n",
    "            if not forward_only and args.glove: #training\n",
    "                #init_embeddings = tf.constant(get_init_embedding(word_dict ,reversed_dict, self.embedding_size), dtype=tf.float32)\n",
    "                init_embeddings = tf.constant(test_embedding, dtype=tf.float32)\n",
    "                \n",
    "            else: #testing\n",
    "                init_embeddings = tf.random_uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0)\n",
    "                # TODO: Handle sentence-textrank matrix case here (Test case)\n",
    "                \n",
    "                \n",
    "            self.embeddings = tf.get_variable(\"embeddings\", initializer=init_embeddings)\n",
    "#             self.sentence_text_rank_mat = tf.get_variable(\"sentence_text_rank_mat\", initializer=sentence_textrank_mat_tf)\n",
    "#             self.encoder_textrank_inp = tf.transpose(tf.nn.embedding_lookup(self.sentence_text_rank_mat, self.sentence_ids), perm=[2, 0, 1])\n",
    "#             self.encoder_textrank_inp = tf.tile(self.encoder_textrank_inp, multiples=[1, 1, 10])\n",
    "            \n",
    "            self.encoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.X), perm=[1, 0, 2])\n",
    "            self.decoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.decoder_input), perm=[1, 0, 2])\n",
    "\n",
    "#             self.encoder_emb_inp = tf.concat([self.encoder_emb_inp, self.encoder_textrank_inp], axis=2)\n",
    "            \n",
    "#             self.my_debug_inp = self.final_encoder_emb_inp\n",
    "            \n",
    "#             self.my_debug_inp2 = self.encoder_emb_inp\n",
    "            \n",
    "\n",
    "        with tf.name_scope(\"encoder\"):\n",
    "            fw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "            bw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "            # TODO: Dropout\n",
    "            fw_cells = [rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob) for cell in fw_cells]\n",
    "            bw_cells = [rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob) for cell in bw_cells]\n",
    "\n",
    "            encoder_outputs, encoder_state_fw, encoder_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "                fw_cells, bw_cells, self.encoder_emb_inp,\n",
    "                sequence_length=self.X_len, time_major=True, dtype=tf.float32)\n",
    "            self.encoder_output = tf.concat(encoder_outputs, 2)\n",
    "            encoder_state_c = tf.concat((encoder_state_fw[0].c, encoder_state_bw[0].c), 1)\n",
    "            encoder_state_h = tf.concat((encoder_state_fw[0].h, encoder_state_bw[0].h), 1)\n",
    "            self.encoder_state = rnn.LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
    "\n",
    "        with tf.name_scope(\"decoder\"), tf.variable_scope(\"decoder\") as decoder_scope:\n",
    "            decoder_cell = self.cell(self.num_hidden * 2)\n",
    "\n",
    "            if not forward_only: #trainig\n",
    "                attention_states = tf.transpose(self.encoder_output, [1, 0, 2])\n",
    "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                    self.num_hidden * 2, attention_states, memory_sequence_length=self.X_len, normalize=True)\n",
    "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
    "                                                                   attention_layer_size=self.num_hidden * 2)\n",
    "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size)\n",
    "                initial_state = initial_state.clone(cell_state=self.encoder_state)\n",
    "                helper = tf.contrib.seq2seq.TrainingHelper(self.decoder_emb_inp, self.decoder_len, time_major=True)\n",
    "                decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, initial_state)\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=True, scope=decoder_scope)\n",
    "                self.decoder_output = outputs.rnn_output\n",
    "                self.logits = tf.transpose(\n",
    "                    self.projection_layer(self.decoder_output), perm=[1, 0, 2])\n",
    "                self.logits_reshape = tf.concat(\n",
    "                    [self.logits, tf.zeros([self.batch_size, summary_max_len - tf.shape(self.logits)[1], self.vocabulary_size])], axis=1)\n",
    "            else: #testing\n",
    "                tiled_encoder_output = tf.contrib.seq2seq.tile_batch(\n",
    "                    tf.transpose(self.encoder_output, perm=[1, 0, 2]), multiplier=self.beam_width)\n",
    "                tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(self.encoder_state, multiplier=self.beam_width)\n",
    "                tiled_seq_len = tf.contrib.seq2seq.tile_batch(self.X_len, multiplier=self.beam_width)\n",
    "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                    self.num_hidden * 2, tiled_encoder_output, memory_sequence_length=tiled_seq_len, normalize=True)\n",
    "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
    "                                                                   attention_layer_size=self.num_hidden * 2)\n",
    "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size * self.beam_width)\n",
    "                initial_state = initial_state.clone(cell_state=tiled_encoder_final_state)\n",
    "                decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "                    cell=decoder_cell,\n",
    "                    embedding=self.embeddings,\n",
    "                    start_tokens=tf.fill([self.batch_size], tf.constant(2)),\n",
    "                    end_token=tf.constant(3),\n",
    "                    initial_state=initial_state,\n",
    "                    beam_width=self.beam_width,\n",
    "                    output_layer=self.projection_layer\n",
    "                )\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    decoder, output_time_major=True, maximum_iterations=summary_max_len, scope=decoder_scope)\n",
    "                self.prediction = tf.transpose(outputs.predicted_ids, perm=[1, 2, 0])\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            if not forward_only: #training\n",
    "                crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    logits=self.logits_reshape, labels=self.decoder_target)\n",
    "                weights = tf.sequence_mask(self.decoder_len, summary_max_len, dtype=tf.float32)\n",
    "                self.loss = tf.reduce_sum(crossent * weights / tf.to_float(self.batch_size))\n",
    "                # TODO: Regularization\n",
    "                params = tf.trainable_variables()\n",
    "                gradients = tf.gradients(self.loss, params)\n",
    "                clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "                optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "                self.update = optimizer.apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_xON6b4nS8x"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ak3tIhETnaIJ"
   },
   "source": [
    "https://github.com/dongjun-Lee/text-summarization-tensorflow/blob/master/train.py\n",
    "\n",
    "We used sumdata/train/train.article.txt and sumdata/train/train.title.txt for training data. To train the model, use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving baseline model in separate directory dataset/saved_model_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vq9ZA0hFnUZJ",
    "outputId": "65296e28-3025-4a7b-a2ec-6227dcffe53b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rmodi/anaconda3/envs/nlp-project/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-23-6e3c343e283b>:64: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/rmodi/anaconda3/envs/nlp-project/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/rnn.py:233: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/rmodi/anaconda3/envs/nlp-project/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/rmodi/anaconda3/envs/nlp-project/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/rmodi/anaconda3/envs/nlp-project/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-23-6e3c343e283b>:126: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration starts.\n",
      "Number of batches per epoch : 3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [05:26,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: loss = 52.30094909667969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [10:54,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2000: loss = 40.82603073120117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [16:18,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3000: loss = 39.40196990966797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3125it [17:03,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1: Model is saved. Elapsed: 00:17:06.36 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [21:52,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4000: loss = 37.55078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [27:32,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5000: loss = 39.0578498840332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6000it [32:59,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6000: loss = 29.89036750793457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6250it [34:58,  9.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 2: Model is saved. Elapsed: 00:34:34.24 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6646it [37:06,  3.15it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-66015293651b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m         }\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# Debugging logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "start = time.perf_counter()\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class args:\n",
    "    pass\n",
    "  \n",
    "args.num_hidden=150\n",
    "args.num_layers=2\n",
    "args.beam_width=10\n",
    "args.glove=\"store_true\"\n",
    "args.embedding_size=320\n",
    "\n",
    "args.learning_rate=1e-3\n",
    "args.batch_size=64\n",
    "args.num_epochs=10\n",
    "args.keep_prob = 0.8\n",
    "\n",
    "args.toy=False #\"store_true\"\n",
    "\n",
    "args.with_model=\"store_true\"\n",
    "\n",
    "\n",
    "#parser = argparse.ArgumentParser()\n",
    "#add_arguments(parser)\n",
    "#args = parser.parse_args()\n",
    "#with open(\"args.pickle\", \"wb\") as f:\n",
    "#    pickle.dump(args, f)\n",
    "\n",
    "if not os.path.exists(default_path + \"saved_model_baseline\"):\n",
    "    os.mkdir(default_path + \"saved_model_baseline\")\n",
    "else:\n",
    "    if args.with_model:\n",
    "        old_model_checkpoint_path = open(default_path + 'saved_model_baseline/checkpoint', 'r')\n",
    "#         old_model_checkpoint_path = \"\".join([default_path + \"saved_model_2/\",old_model_checkpoint_path.read().splitlines()[0].split('\"')[1] ])\n",
    "#  abs Change made by paras\n",
    "        old_model_checkpoint_path = old_model_checkpoint_path.read().splitlines()[0].split('\"')[1]\n",
    "\n",
    "\n",
    "#print(\"Building dictionary...\")\n",
    "#word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"train\", args.toy)\n",
    "#print(\"Loading training dataset...\")\n",
    "#train_x, train_y = build_dataset(\"train\", word_dict, article_max_len, summary_max_len, args.toy)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model = Model(reversed_dict, article_max_len, summary_max_len, args)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    " #   if 'old_model_checkpoint_path' in globals():\n",
    " #       print(\"Continuing from previous trained model:\" , old_model_checkpoint_path , \"...\")\n",
    " #       saver.restore(sess, old_model_checkpoint_path )\n",
    "\n",
    "    batches = batch_iter(train_x, train_y, args.batch_size, args.num_epochs)\n",
    "    num_batches_per_epoch = (len(train_x) - 1) // args.batch_size + 1\n",
    "\n",
    "    print(\"\\nIteration starts.\")\n",
    "    print(\"Number of batches per epoch :\", num_batches_per_epoch)\n",
    "    total_batches_done = 0\n",
    "    \n",
    "    \n",
    "    for batch_x, batch_y, sentence_ids in tqdm(batches):\n",
    "        batch_x_len = list(map(lambda x: len([y for y in x if y != 0]), batch_x))\n",
    "        batch_decoder_input = list(map(lambda x: [word_dict[\"<s>\"]] + list(x), batch_y))\n",
    "        batch_decoder_len = list(map(lambda x: len([y for y in x if y != 0]), batch_decoder_input))\n",
    "        batch_decoder_output = list(map(lambda x: list(x) + [word_dict[\"</s>\"]], batch_y))\n",
    "\n",
    "        batch_decoder_input = list(\n",
    "            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_input))\n",
    "        batch_decoder_output = list(\n",
    "            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_output))\n",
    "        \n",
    "#         Debugging steps\n",
    "#         print(\"Sentence id len: \" + str(len(sentence_ids)))\n",
    "#         print(\"Total batches done: \" + str(total_batches_done))\n",
    "        total_batches_done += 1\n",
    "#         print(\"Batch len: \" + str(len(batch_x)))\n",
    "#         print(\"Batch: \" + str(batch_x))\n",
    "#         print(\"Sentence: \" + str(sentence_ids))\n",
    "        \n",
    "        train_feed_dict = {\n",
    "            model.batch_size: len(batch_x),\n",
    "            model.X: batch_x,\n",
    "            model.X_len: batch_x_len,\n",
    "            model.decoder_input: batch_decoder_input,\n",
    "            model.decoder_len: batch_decoder_len,\n",
    "            model.decoder_target: batch_decoder_output,\n",
    "            model.sentence_ids: sentence_ids\n",
    "        }\n",
    "\n",
    "        _, step, loss = sess.run([model.update, model.global_step, model.loss], feed_dict=train_feed_dict)\n",
    "        \n",
    "        # Debugging logs         \n",
    "#         print(str(model.my_debug_inp.get_shape()) + \" logging shape\")\n",
    "#         print(str(model.my_debug_inp2.get_shape()) + \" logging shape\")\n",
    "        \n",
    "#         tf.Print(model.encoder_textrank_inp, [model.encoder_textrank_inp], message=\"Textrank input \")\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(\"step {0}: loss = {1}\".format(step, loss))\n",
    "\n",
    "        if step % num_batches_per_epoch == 0:\n",
    "            hours, rem = divmod(time.perf_counter() - start, 3600)\n",
    "            minutes, seconds = divmod(rem, 60)\n",
    "            saver.save(sess, default_path + \"saved_model_baseline/model.ckpt\", global_step=step)\n",
    "            print(\" Epoch {0}: Model is saved.\".format(step // num_batches_per_epoch),\n",
    "            \"Elapsed: {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds) , \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lkWJfa_CYjPo"
   },
   "source": [
    "last training was \n",
    "```\n",
    "step 47000: loss = 8.033841133117676\n",
    "step 48000: loss = 9.481734275817871\n",
    "step 49000: loss = 7.188093662261963\n",
    "step 50000: loss = 14.354914665222168\n",
    "Epoch 16: Model is saved. Elapsed: 02:19:23.39\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache miss percentage: 0.8633910300721934 %\n"
     ]
    }
   ],
   "source": [
    "sentence_textrank_valid_mat = build_sentence_textrank_mat(\"valid\", word_dict, article_max_len, summary_max_len, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.875915765762329"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(sentence_textrank_valid_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_textrank_valid_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sentence_textrank_valid_mat.shape[0] < textrank_train_len:\n",
    "    padding_rows = np.zeros((textrank_train_len - sentence_textrank_valid_mat.shape[0], sentence_textrank_valid_mat.shape[1]))\n",
    "    sentence_textrank_valid_mat = np.concatenate([sentence_textrank_valid_mat, padding_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 50)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_textrank_valid_mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TbbtwLe7njGS"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iCBIdwESnlhZ"
   },
   "source": [
    "https://github.com/dongjun-Lee/text-summarization-tensorflow/blob/master/test.py\n",
    "\n",
    "Generate summary of each article in sumdata/train/valid.article.filter.txt by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "1IUJ0dpon1Hi",
    "outputId": "091d9f83-9db1-4594-f1ed-8398a26535bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dictionary...\n",
      "Loading validation dataset...\n",
      "Loading article and reference...\n",
      "Loading saved model...\n",
      "WARNING:tensorflow:From /home/rmodi/anaconda3/envs/nlp-project/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py:733: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/rmodi/anaconda3/envs/nlp-project/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /media/mount/Users/Ruchit Modi/Documents/CSE538/project/Model2/dataset/saved_model_baseline/model.ckpt-6250\n",
      "Writing summaries to 'result2.txt'...\n",
      "Summaries have been generated\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "#from model import Model\n",
    "#from utils import build_dict, build_dataset, batch_iter\n",
    "\n",
    "\n",
    "#with open(\"args.pickle\", \"rb\") as f:\n",
    "#    args = pickle.load(f)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "class args:\n",
    "    pass\n",
    "  \n",
    "args.num_hidden=150\n",
    "args.num_layers=2\n",
    "args.beam_width=10\n",
    "args.glove=\"store_true\"\n",
    "args.embedding_size=320\n",
    "\n",
    "args.learning_rate=1e-3\n",
    "args.batch_size=64\n",
    "args.num_epochs=10\n",
    "args.keep_prob = 0.8\n",
    "\n",
    "args.toy=True\n",
    "\n",
    "args.with_model=\"store_true\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Loading dictionary...\")\n",
    "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"valid\", args.toy)\n",
    "print(\"Loading validation dataset...\")\n",
    "valid_x = build_dataset(\"valid\", word_dict, article_max_len, summary_max_len, args.toy)\n",
    "valid_x_len = [len([y for y in x if y != 0]) for x in valid_x]\n",
    "print(\"Loading article and reference...\")\n",
    "article = get_text_list(valid_article_path, args.toy)\n",
    "reference = get_text_list(valid_title_path, args.toy)\n",
    "\n",
    "\n",
    "f = open(default_path + \"baseline_results.txt\", \"w\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"Loading saved model...\")\n",
    "    model = Model(reversed_dict, article_max_len, summary_max_len, args, forward_only=True)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    ckpt = tf.train.get_checkpoint_state(default_path + \"saved_model_baseline/\")\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    batches = batch_iter(valid_x, [0] * len(valid_x), args.batch_size, 1)\n",
    "\n",
    "    print(\"Writing summaries to 'baseline_results.txt'...\")\n",
    "    i = 0\n",
    "    for batch_x, _, sentence_ids in batches:\n",
    "        batch_x_len = [len([y for y in x if y != 0]) for x in batch_x]\n",
    "\n",
    "        valid_feed_dict = {\n",
    "            model.batch_size: len(batch_x),\n",
    "            model.X: batch_x,\n",
    "            model.X_len: batch_x_len,\n",
    "            model.sentence_ids: sentence_ids\n",
    "        }\n",
    "\n",
    "        prediction = sess.run(model.prediction, feed_dict=valid_feed_dict)\n",
    "        prediction_output = [[reversed_dict[y] for y in x] for x in prediction[:, 0, :]]\n",
    "        summary_array = []\n",
    "        j = 0\n",
    "        for line in prediction_output:\n",
    "            summary = list()\n",
    "            for word in line:\n",
    "                if word == \"</s>\":\n",
    "                    break\n",
    "                if word not in summary:\n",
    "                    summary.append(word)\n",
    "            summary_array.append(\" \".join(summary))\n",
    "            write_line = '==============>>>>>>>>'.join([article[i+j], \" \".join(summary)]) + '\\n'\n",
    "            separator = \"===========================================\"\n",
    "            write_line = article[i+j] + \"\\n\" + separator + \"\\n\" + reference[i+j] + \"\\n\" + separator + \"\\n\" +  \" \".join(summary) + \"\\n\"\n",
    "            write_line = write_line + \"++++++++++++++++++++++++++++++++++++++++++++++++++++\" + \"\\n\" + \"\\n\"\n",
    "            f.write(write_line)\n",
    "            j += 1\n",
    "            #print(\" \".join(summary), file=f)\n",
    "        i += len(batch_x)\n",
    "\n",
    "    print('Summaries have been generated')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['philippine shares close # percent lower',\n",
       " 'pope of america on us embassy in yemen',\n",
       " '# dead in yemen',\n",
       " 'philippine shares close # percent lower',\n",
       " 'south zealand shares plunge # percent',\n",
       " \"pope 's lavrov to return in iraq\",\n",
       " 'south african pm leader to meet deal',\n",
       " '# dead in yemen',\n",
       " 'malaysian shares close # percent on dollars',\n",
       " 'south zealand shares close # percent',\n",
       " '# dead -bln-dlr dollars in august',\n",
       " '# dead billion dollars in yemen',\n",
       " 'five dead # -bln-dlr in yemen',\n",
       " '# killed billion dollars in yemen',\n",
       " 'new zealand shares plunge # percent',\n",
       " '# dead billion dollars in yemen']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WZl_ZCQpRMRt"
   },
   "outputs": [],
   "source": [
    "# summary_array = []\n",
    "# with open(default_path + \"result2.txt\", \"w\") as f:\n",
    "#     for line in prediction_output:\n",
    "#         summary = list()\n",
    "#         for word in line:\n",
    "#             if word == \"</s>\":\n",
    "#                 break\n",
    "#             if word not in summary:\n",
    "#                 summary.append(word)\n",
    "#         summary_array.append(\" \".join(summary))\n",
    "#         f.write(\" \".join(summary) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "fELp35RzRd1w",
    "outputId": "f78cafcc-8a1b-4320-8a64-149ab2a38aad"
   },
   "outputs": [],
   "source": [
    "# summary_array[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K-Kj9e5M9lBr"
   },
   "source": [
    "## Evaluate & write output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svL3XWnXaTsW"
   },
   "source": [
    "for comparing (good resources)\n",
    "\n",
    "1.  [thunlp]( https://github.com/thunlp/TensorFlow-Summarization)     works with duc2003\n",
    "2.  [textsum](https://github.com/tensorflow/models/tree/master/research/textsum )   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "x6nERCu29oPS",
    "outputId": "6057d7e7-1e09-4f08-d051-f23af726e2fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.stocks=(nsubj)=>close\n",
      "<BasicElement: stocks-[nsubj]->close>\n",
      "a.percent=(dobj)=>close\n",
      "<BasicElement: percent-[dobj]->close>\n",
      "b.fresh=(amod)=>record\n",
      "a.high=(advmod)=>close\n",
      "<BasicElement: high-[advmod]->close>\n",
      "a.shares=(nsubj)=>close\n",
      "<BasicElement: shares-[nsubj]->close>\n",
      "a.percent=(dobj)=>close\n",
      "<BasicElement: percent-[dobj]->close>\n",
      "ROUGE-1: 0.5454545454545454\n",
      "ROUGE-2: 0.2222222222222222\n",
      "ROUGE-L: 0.5454545454545454\n",
      "ROUGE-BE: 0.3333333333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5454545454545454,\n",
       " 0.2222222222222222,\n",
       " 0.5454545454545454,\n",
       " 0.3333333333333333,\n",
       " 7.386099955930605)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://github.com/chakki-works/sumeval\n",
    "#https://github.com/Tian312/awesome-text-summarization\n",
    "\n",
    "from sumeval.metrics.rouge import RougeCalculator\n",
    "from sumeval.metrics.bleu import BLEUCalculator\n",
    "\n",
    "def eval_rouges(refrence_summary,model_summary):\n",
    "    refrence_summary = \"tokyo shares close up #.## percent\"\n",
    "    model_summary = \"tokyo stocks close up # percent to fresh record high\"\n",
    "\n",
    "    rouge = RougeCalculator(stopwords=True, lang=\"en\")\n",
    "\n",
    "    rouge_1 = rouge.rouge_n(\n",
    "                summary=model_summary,\n",
    "                references=refrence_summary,\n",
    "                n=1)\n",
    "\n",
    "    rouge_2 = rouge.rouge_n(\n",
    "                summary=model_summary,\n",
    "                references=[refrence_summary],\n",
    "                n=2)\n",
    "    \n",
    "    rouge_l = rouge.rouge_l(\n",
    "                summary=model_summary,\n",
    "                references=[refrence_summary])\n",
    "    \n",
    "    # You need spaCy to calculate ROUGE-BE\n",
    "    \n",
    "    rouge_be = rouge.rouge_be(\n",
    "                summary=model_summary,\n",
    "                references=[refrence_summary])\n",
    "\n",
    "    bleu = BLEUCalculator()\n",
    "    bleu_score = bleu.bleu( summary=model_summary,\n",
    "                        references=[refrence_summary])\n",
    "\n",
    "    print(\"ROUGE-1: {}, ROUGE-2: {}, ROUGE-L: {}, ROUGE-BE: {}\".format(\n",
    "        rouge_1, rouge_2, rouge_l, rouge_be\n",
    "    ).replace(\", \", \"\\n\"))\n",
    "    \n",
    "    return rouge_1, rouge_2,rouge_l,rouge_be,bleu_score\n",
    "eval_rouges('a','b')\n",
    "#rouge_1, rouge_2,rouge_l,rouge_be = eval_rouges( \"tokyo shares close up #.## percent\",  \"tokyo stocks close up # percent to fresh record high\")\n",
    "\n",
    "#print(\"ROUGE-1: {}, ROUGE-2: {}, ROUGE-L: {}, ROUGE-BE: {}\".format(     rouge_1, rouge_2, rouge_l, rouge_be).replace(\", \", \"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "EyKcd5ffKEBJ",
    "outputId": "723bd299-b702-4dd2-b0a2-d52d1cda73d9"
   },
   "outputs": [],
   "source": [
    "#https://pymotw.com/2/xml/etree/ElementTree/create.html\n",
    "\n",
    "bleu_arr = []\n",
    "rouge_1_arr  = []\n",
    "rouge_2_arr  = []\n",
    "rouge_L_arr  = []\n",
    "rouge_be_arr = []\n",
    "\n",
    "from xml.etree import ElementTree\n",
    "from xml.dom import minidom\n",
    "from functools import reduce\n",
    "\n",
    "def prettify(elem):\n",
    "    \"\"\"Return a pretty-printed XML string for the Element.\n",
    "    \"\"\"\n",
    "    rough_string = ElementTree.tostring(elem, 'utf-8')\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    return reparsed.toprettyxml(indent=\"  \")\n",
    "  \n",
    "from xml.etree.ElementTree import Element, SubElement, Comment\n",
    "\n",
    "top = Element('ZakSum')\n",
    "\n",
    "comment = Comment('Generated by Amr Zaki')\n",
    "top.append(comment)\n",
    "\n",
    "i=0\n",
    "for summ in summary_array:\n",
    "    example = SubElement(top, 'example')\n",
    "    article_element   = SubElement(example, 'article')\n",
    "    article_element.text = article[i]\n",
    "\n",
    "    reference_element = SubElement(example, 'reference')\n",
    "    reference_element.text = reference[i]\n",
    "\n",
    "    summary_element   = SubElement(example, 'summary')\n",
    "    summary_element.text = summ\n",
    "\n",
    "    rouge_1, rouge_2,rouge_L,rouge_be,bleu_score = eval_rouges(reference[i],summ )\n",
    "\n",
    "    eval_element = SubElement(example, 'eval')\n",
    "    bleu_score_element = SubElement(eval_element,'BLEU', {'score':str(bleu_score)})\n",
    "    ROUGE_1_element  = SubElement(eval_element, 'ROUGE_1' , {'score':str(rouge_1)})\n",
    "    ROUGE_2_element  = SubElement(eval_element, 'ROUGE_2' , {'score':str(rouge_2)})\n",
    "    ROUGE_L_element  = SubElement(eval_element, 'ROUGE_l' , {'score':str(rouge_L)})\n",
    "    ROUGE_be_element  = SubElement(eval_element,'ROUGE_be', {'score':str(rouge_be)})\n",
    "\n",
    "    bleu_arr.append(bleu_score) \n",
    "    rouge_1_arr.append(rouge_1) \n",
    "    rouge_2_arr.append(rouge_2) \n",
    "    rouge_L_arr.append(rouge_L) \n",
    "    rouge_be_arr.append(rouge_be) \n",
    "\n",
    "    i+=1\n",
    "\n",
    "top.set('bleu', str(reduce(lambda x, y: x + y,  bleu_arr) / len(bleu_arr)))\n",
    "top.set('rouge_1', str(reduce(lambda x, y: x + y,  rouge_1_arr) / len(rouge_1_arr)))\n",
    "top.set('rouge_2', str(reduce(lambda x, y: x + y,  rouge_2_arr) / len(rouge_2_arr)))\n",
    "top.set('rouge_L', str(reduce(lambda x, y: x + y,  rouge_L_arr) / len(rouge_L_arr)))\n",
    "top.set('rouge_be', str(reduce(lambda x, y: x + y, rouge_be_arr) / len(rouge_be_arr)))\n",
    "\n",
    "with open(default_path + \"result_featurerich_15_11_2018_5_28pm.xml\", \"w+\") as f:\n",
    "    print(prettify(top), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ihl49sb7bIRv",
    "outputId": "40e1f2f6-fe6b-4c23-a141-824eb83f79c2"
   },
   "outputs": [],
   "source": [
    "# len(summary_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FvnBdHIpb-wo"
   },
   "outputs": [],
   "source": [
    "sentence = \"The Lion Air flight JT 610 had been carrying 189 people, including three children, when it disappeared from radar during a short flight from Jakarta to Pangkal Pinang on the Indonesian island of Bangka, according to Basarnas, Indonesia's national search and rescue agency.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text-Summarization-2-features-paper(tf-idf , pos tags)",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Conda: nlp-project",
   "language": "python",
   "name": "nlp-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
