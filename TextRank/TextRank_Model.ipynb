{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/38088652/pandas-convert-categories-to-numbers\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ex = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'\n",
    "\n",
    "def get_pos_tags_dict(words):\n",
    "    #sent = nltk.word_tokenize(sent)\n",
    "    #print(sent)\n",
    "    post_tags_for_words = nltk.pos_tag(words)\n",
    "\n",
    "    pos_list ={}\n",
    "    #sent = preprocess(ex)\n",
    "    for word,pos in post_tags_for_words:\n",
    "        pos_list[word] = pos\n",
    "    #print(pos_list)\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(list(pos_list.items()))\n",
    "    df.columns = ['word', 'pos']\n",
    "    df.pos = pd.Categorical(df.pos)\n",
    "    df['code'] = df.pos.cat.codes\n",
    "    #print(df)\n",
    "\n",
    "    pos_list ={}\n",
    "    for index, row in df.iterrows():\n",
    "        pos_list[row['word']] = row['code']\n",
    "#     print(pos_list)\n",
    "    return pos_list , post_tags_for_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[231]:\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import collections\n",
    "import pickle\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# default_path = \"drive/My Drive/NLP/project/rnn-lstm/data/\"\n",
    "default_path = \"/media/mount/Users/Ruchit Modi/Documents/CSE538/project/Model2/dataset/\"\n",
    "textrank_base_path = \"/media/mount/Users/Ruchit Modi/Documents/CSE538/project/Model2\"\n",
    "\n",
    "train_article_path = default_path + \"sumdata/train/train.article.txt\"\n",
    "train_title_path   = default_path + \"sumdata/train/train.title.txt\"\n",
    "valid_article_path = default_path + \"sumdata/train/valid.article.filter.txt\"\n",
    "valid_title_path   = default_path + \"sumdata/train/valid.title.filter.txt\"\n",
    "\n",
    "textrank_train_len = 200000\n",
    "textrank_valid_len = 50\n",
    "\n",
    "def clean_str(sentence):\n",
    "    sentence = re.sub(\"[#.]+\", \"#\", sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def get_text_list(data_path, toy):\n",
    "    with open (data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        if not toy:\n",
    "            return [clean_str(x.strip()) for x in f.readlines()][:textrank_train_len]\n",
    "        else:\n",
    "            return [clean_str(x.strip()) for x in f.readlines()][:textrank_valid_len]\n",
    "        \n",
    "def build_dict(step, toy=False):\n",
    "    if step == \"train\":\n",
    "        train_article_list = get_text_list(train_article_path, toy)\n",
    "        train_title_list = get_text_list(train_title_path, toy)\n",
    "\n",
    "        words = list()\n",
    "        for sentence in train_article_list + train_title_list:\n",
    "            for word in word_tokenize(sentence):\n",
    "                words.append(word)\n",
    "\n",
    "        word_counter = collections.Counter(words).most_common()\n",
    "        word_dict = dict()\n",
    "        word_dict[\"<padding>\"] = 0\n",
    "        word_dict[\"<unk>\"] = 1\n",
    "        word_dict[\"<s>\"] = 2\n",
    "        word_dict[\"</s>\"] = 3\n",
    "        for word, _ in word_counter:\n",
    "            word_dict[word] = len(word_dict)\n",
    "\n",
    "        with open(default_path + \"word_dict.pickle\", \"wb\") as f:\n",
    "            pickle.dump(word_dict, f)\n",
    "\n",
    "    elif step == \"valid\":\n",
    "        with open(default_path + \"word_dict.pickle\", \"rb\") as f:\n",
    "            word_dict = pickle.load(f)\n",
    "\n",
    "    reversed_dict = dict(zip(word_dict.values(), word_dict.keys()))\n",
    "\n",
    "    article_max_len = 50\n",
    "    summary_max_len = 15\n",
    "\n",
    "    return word_dict, reversed_dict, article_max_len, summary_max_len\n",
    "\n",
    "\n",
    "def build_dataset(step, word_dict, article_max_len, summary_max_len, toy=False):\n",
    "    if step == \"train\":\n",
    "        article_list = get_text_list(train_article_path, toy)\n",
    "        title_list = get_text_list(train_title_path, toy)\n",
    "    elif step == \"valid\":\n",
    "        # Only when args.toy == True\n",
    "        article_list = get_text_list(valid_article_path, toy)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    x = [word_tokenize(d) for d in article_list]\n",
    "    x = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in d] for d in x]\n",
    "    x = [d[:article_max_len] for d in x]\n",
    "    x = [d + (article_max_len - len(d)) * [word_dict[\"<padding>\"]] for d in x]\n",
    "    \n",
    "    if step == \"valid\":\n",
    "        return x\n",
    "    else:        \n",
    "        y = [word_tokenize(d) for d in title_list]\n",
    "        y = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in d] for d in y]\n",
    "        y = [d[:(summary_max_len - 1)] for d in y]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def batch_iter(inputs, outputs, batch_size, num_epochs):\n",
    "    inputs = np.array(inputs)\n",
    "    outputs = np.array(outputs)\n",
    "\n",
    "    sentence_ids = [[i] for i in range(inputs.shape[0])]\n",
    "    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, len(inputs))\n",
    "            yield inputs[start_index:end_index], outputs[start_index:end_index], sentence_ids[start_index:end_index]\n",
    "\n",
    "\n",
    "def get_init_embedding(word_dict , reversed_dict, embedding_size):\n",
    "    print(\"Loading Lists...\")\n",
    "    train_article_list = get_text_list(train_article_path, False)\n",
    "    train_title_list = get_text_list(train_title_path, False)\n",
    "\n",
    "    print(\"Loading TF-IDF...\")\n",
    "    tf_idf_list = tf_idf_generate(train_article_list+train_title_list)\n",
    "    \n",
    "    print(\"Loading Pos Tags...\")\n",
    "    pos_list , postags_for_named_entity = get_pos_tags_dict(word_dict.keys())\n",
    "\n",
    "    #print(\"Loading Named Entity...\")\n",
    "    #named_entity_recs = named_entity(postags_for_named_entity) \n",
    "    \n",
    "    print(\"Loading Glove vectors...\")\n",
    "\n",
    "    with open( default_path + \"glove/model_glove_300.pkl\", 'rb') as handle:\n",
    "        word_vectors = pickle.load(handle)     \n",
    "    \n",
    "    used_words = 0\n",
    "    word_vec_list = list()\n",
    "    for _, word in sorted(reversed_dict.items()):\n",
    "        try:\n",
    "            word_vec = word_vectors.word_vec(word)\n",
    "            if word in tf_idf_list:\n",
    "                v= tf_idf_list[word]\n",
    "                rich_feature_array = np.array([v,v,v,v,v,v,v,v,v,v])\n",
    "                word_vec = np.append(word_vec, rich_feature_array)\n",
    "            else:\n",
    "                v=0\n",
    "                rich_feature_array = np.array([v,v,v,v,v,v,v,v,v,v])\n",
    "                word_vec = np.append(word_vec, rich_feature_array)\n",
    "\n",
    "            if word in pos_list:\n",
    "                v=pos_list[word]\n",
    "                rich_feature_array_2 = np.array([v,v,v,v,v,v,v,v,v,v])\n",
    "                word_vec = np.append(word_vec, rich_feature_array_2)\n",
    "            else:\n",
    "                v=0\n",
    "                rich_feature_array_2 = np.array([v,v,v,v,v,v,v,v,v,v])\n",
    "                word_vec = np.append(word_vec, rich_feature_array_2) \n",
    "          \n",
    "            used_words += 1\n",
    "        except KeyError:\n",
    "            word_vec = np.zeros([embedding_size], dtype=np.float32) #to generate for <padding> and <unk>\n",
    "        \n",
    "        \n",
    "        word_vec_list.append(np.array(word_vec))\n",
    "\n",
    "    print(\"words found in glove percentage = \" + str((used_words/len(word_vec_list))*100) )\n",
    "          \n",
    "    # Assign random vector to <s>, </s> token\n",
    "    word_vec_list[2] = np.random.normal(0, 1, embedding_size)\n",
    "    word_vec_list[3] = np.random.normal(0, 1, embedding_size)\n",
    "\n",
    "    return np.array(word_vec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n",
      "Loading training dataset...\n",
      "Building pos-tags\n",
      "Loading Lists...\n",
      "Loading TF-IDF...\n",
      "Currently assuming: article_max_len : 50\n",
      "Currently assuming: summary_max_len : 15\n",
      "Building input embeddings...\n",
      "Loading Lists...\n",
      "Loading TF-IDF...\n",
      "Loading Pos Tags...\n",
      "Loading Glove vectors...\n",
      "words found in glove percentage = 91.75771029889796\n"
     ]
    }
   ],
   "source": [
    "# In[232]:\n",
    "\n",
    "\n",
    "# _____TF-IDF libraries_____\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# _____helper Libraries_____\n",
    "import pickle  # would be used for saving temp files\n",
    "import csv     # used for accessing the dataset\n",
    "import timeit  # to measure time of training\n",
    "import random  # used to get a random number\n",
    "\n",
    "\n",
    "def tf_idf_generate(sentences):\n",
    "    #https://stackoverflow.com/questions/30976120/find-the-tf-idf-score-of-specific-words-in-documents-using-sklearn\n",
    "\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "    # our corpus\n",
    "    data = sentences\n",
    "\n",
    "    cv = CountVectorizer()\n",
    "\n",
    "    # convert text data into term-frequency matrix\n",
    "    data = cv.fit_transform(data)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "    # convert term-frequency matrix into tf-idf\n",
    "    tfidf_matrix = tfidf_transformer.fit_transform(data)\n",
    "\n",
    "    # create dictionary to find a tfidf word each word\n",
    "    word2tfidf = dict(zip(cv.get_feature_names(), tfidf_transformer.idf_))\n",
    "  \n",
    "    return word2tfidf\n",
    "\n",
    "# https://nlpforhackers.io/named-entity-extraction/\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "\n",
    "def named_entity(post_tags_for_words):\n",
    "    names = ne_chunk(post_tags_for_words)\n",
    "    names_dict = {}\n",
    "    for n in names:\n",
    "        if (len(n) == 1):\n",
    "            named_entity = str(n).split(' ')[0][1:]\n",
    "            word = str(n).split(' ')[1].split('/')[0]\n",
    "            names_dict[word] = named_entity\n",
    "    print(names_dict)\n",
    "\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.DataFrame(list(names_dict.items()))\n",
    "    df.columns = ['word', 'pos']\n",
    "    df.pos = pd.Categorical(df.pos)\n",
    "    df['code'] = df.pos.cat.codes\n",
    "    # print(df)\n",
    "\n",
    "    names_dict = {}\n",
    "    for index, row in df.iterrows():\n",
    "        names_dict[row['word']] = row['code']\n",
    "    print(names_dict)\n",
    "    return names_dict\n",
    "\n",
    "\n",
    "\n",
    "## Generating dependencies\n",
    "\n",
    "print(\"Building dictionary...\")\n",
    "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"train\", False)\n",
    "\n",
    "print(\"Loading training dataset...\")\n",
    "train_x, train_y = build_dataset(\"train\", word_dict, article_max_len, summary_max_len, False)\n",
    "\n",
    "print(\"Building pos-tags\")\n",
    "pos_list , postags_for_named_entity = get_pos_tags_dict(word_dict.keys())\n",
    "\n",
    "print(\"Loading Lists...\")\n",
    "train_article_list = get_text_list(train_article_path, False)\n",
    "train_title_list = get_text_list(train_title_path, False)\n",
    "\n",
    "print(\"Loading TF-IDF...\")\n",
    "tf_idf_list = tf_idf_generate(train_article_list+train_title_list)\n",
    "    \n",
    "print(\"Currently assuming: article_max_len : \" + str(article_max_len))\n",
    "print(\"Currently assuming: summary_max_len : \" + str(summary_max_len))\n",
    "\n",
    "print(\"Building input embeddings...\")\n",
    "input_embedding = get_init_embedding(word_dict , reversed_dict, 320)\n",
    "\n",
    "def generate_article_scores(step, toy=False):\n",
    "    sentence_to_word_dic = {}\n",
    "    cache_miss = 0\n",
    "    total_found = 0\n",
    "    f = open(textrank_base_path + '/{}_article_scores.pickle'.format(step), 'rb')\n",
    "    import pickle\n",
    "    article_text_rank_scores = pickle.load(f)\n",
    "    for article_id in article_text_rank_scores:\n",
    "        word_dic = {}\n",
    "        for word, score in article_text_rank_scores[article_id]:\n",
    "            word_id = word_dict.get(word, None)\n",
    "            if word_id is not None:\n",
    "                word_dic[word_id] = score\n",
    "                total_found += 1\n",
    "            else:\n",
    "                cache_miss += 1\n",
    "        sentence_to_word_dic[article_id] = word_dic\n",
    "    print('Cache miss percentage: {} %'.format(100*cache_miss/(cache_miss + total_found)))\n",
    "    return sentence_to_word_dic\n",
    "\n",
    "\n",
    "def build_train_sentence_textrank_mat(step, word_dict, article_max_len, summary_max_len, toy=False):\n",
    "    if step == \"train\":\n",
    "        article_list = get_text_list(train_article_path, toy)\n",
    "    elif step == \"valid\":\n",
    "        article_list = get_text_list(valid_article_path, toy)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    sentence_to_word_dic = generate_article_scores(step, toy)\n",
    "\n",
    "    x = [word_tokenize(d) for d in article_list]\n",
    "    sentence_mat_x = []\n",
    "    for idx, sentence in enumerate(x):\n",
    "        word_textrank_dic = sentence_to_word_dic[idx]\n",
    "        word_scores = []\n",
    "        for token in sentence:\n",
    "            word_scores.append(word_textrank_dic.get(word_dict.get(token, None), 0.0))\n",
    "        \n",
    "        word_scores = word_scores[:article_max_len]\n",
    "        word_scores = word_scores + [0.0] * (article_max_len - len(word_scores))\n",
    "        sentence_mat_x.append(np.array(word_scores))\n",
    "   \n",
    "    return np.array(sentence_mat_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building training sentence textrank matrix\")\n",
    "train_sentence_textrank_mat = build_train_sentence_textrank_mat(\"train\", word_dict, article_max_len, summary_max_len, False)\n",
    "\n",
    "print(\"Max value in train_sentence_textrank_mat: \" + str(np.max(train_sentence_textrank_mat)) + \" Shape: \" + str(train_sentence_textrank_mat.shape))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "#from utils import get_init_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, reversed_dict, article_max_len, summary_max_len, args, forward_only=False):\n",
    "        self.vocabulary_size = len(reversed_dict)\n",
    "        self.embedding_size = args.embedding_size\n",
    "        self.num_hidden = args.num_hidden\n",
    "        self.num_layers = args.num_layers\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.beam_width = args.beam_width\n",
    "        if not forward_only:\n",
    "            self.keep_prob = args.keep_prob\n",
    "        else:\n",
    "            self.keep_prob = 1.0\n",
    "        self.cell = tf.nn.rnn_cell.BasicLSTMCell\n",
    "        with tf.variable_scope(\"decoder/projection\"):\n",
    "            self.projection_layer = tf.layers.Dense(self.vocabulary_size, use_bias=False)\n",
    "\n",
    "        self.batch_size = tf.placeholder(tf.int32, (), name=\"batch_size\")\n",
    "        self.X = tf.placeholder(tf.int32, [None, article_max_len])\n",
    "        self.X_len = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        self.sentence_ids = tf.placeholder(tf.int32, [None, 1])\n",
    "        \n",
    "        self.decoder_input = tf.placeholder(tf.int32, [None, summary_max_len])\n",
    "        self.decoder_len = tf.placeholder(tf.int32, [None])\n",
    "        self.decoder_target = tf.placeholder(tf.int32, [None, summary_max_len])\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        # Debugging variables\n",
    "        self.my_debug_inp = None\n",
    "        self.my_debug_inp2 = None\n",
    "        \n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            train_sentence_textrank_mat_tf = tf.constant(train_sentence_textrank_mat, dtype=tf.float32)\n",
    "            if not forward_only and args.glove: #training\n",
    "                #init_embeddings = tf.constant(get_init_embedding(word_dict ,reversed_dict, self.embedding_size), dtype=tf.float32)\n",
    "                init_embeddings = tf.constant(input_embedding, dtype=tf.float32)\n",
    "                \n",
    "            else: #testing\n",
    "                init_embeddings = tf.random_uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0)\n",
    "                # TODO: Handle sentence-textrank matrix case here (Test case)\n",
    "                \n",
    "                \n",
    "            self.embeddings = tf.get_variable(\"embeddings\", initializer=init_embeddings)\n",
    "#             self.sentence_text_rank_mat = tf.get_variable(\"sentence_text_rank_mat\", initializer=train_sentence_textrank_mat_tf)\n",
    "#             self.encoder_textrank_inp = tf.transpose(tf.nn.embedding_lookup(self.sentence_text_rank_mat, self.sentence_ids), perm=[2, 0, 1])\n",
    "#             self.encoder_textrank_inp = tf.tile(self.encoder_textrank_inp, multiples=[1, 1, 10])\n",
    "            \n",
    "            self.encoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.X), perm=[1, 0, 2])\n",
    "            self.decoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.decoder_input), perm=[1, 0, 2])\n",
    "\n",
    "#             self.encoder_emb_inp = tf.concat([self.encoder_emb_inp, self.encoder_textrank_inp], axis=2)\n",
    "            \n",
    "#             self.my_debug_inp = self.final_encoder_emb_inp\n",
    "            \n",
    "#             self.my_debug_inp2 = self.encoder_emb_inp\n",
    "            \n",
    "\n",
    "        with tf.name_scope(\"encoder\"):\n",
    "            fw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "            bw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "            # TODO: Dropout\n",
    "            fw_cells = [rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob) for cell in fw_cells]\n",
    "            bw_cells = [rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob) for cell in bw_cells]\n",
    "\n",
    "            encoder_outputs, encoder_state_fw, encoder_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "                fw_cells, bw_cells, self.encoder_emb_inp,\n",
    "                sequence_length=self.X_len, time_major=True, dtype=tf.float32)\n",
    "            self.encoder_output = tf.concat(encoder_outputs, 2)\n",
    "            encoder_state_c = tf.concat((encoder_state_fw[0].c, encoder_state_bw[0].c), 1)\n",
    "            encoder_state_h = tf.concat((encoder_state_fw[0].h, encoder_state_bw[0].h), 1)\n",
    "            self.encoder_state = rnn.LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
    "\n",
    "        with tf.name_scope(\"decoder\"), tf.variable_scope(\"decoder\") as decoder_scope:\n",
    "            decoder_cell = self.cell(self.num_hidden * 2)\n",
    "\n",
    "            if not forward_only: #trainig\n",
    "                attention_states = tf.transpose(self.encoder_output, [1, 0, 2])\n",
    "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                    self.num_hidden * 2, attention_states, memory_sequence_length=self.X_len, normalize=True)\n",
    "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
    "                                                                   attention_layer_size=self.num_hidden * 2)\n",
    "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size)\n",
    "                initial_state = initial_state.clone(cell_state=self.encoder_state)\n",
    "                helper = tf.contrib.seq2seq.TrainingHelper(self.decoder_emb_inp, self.decoder_len, time_major=True)\n",
    "                decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, initial_state)\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=True, scope=decoder_scope)\n",
    "                self.decoder_output = outputs.rnn_output\n",
    "                self.logits = tf.transpose(\n",
    "                    self.projection_layer(self.decoder_output), perm=[1, 0, 2])\n",
    "                self.logits_reshape = tf.concat(\n",
    "                    [self.logits, tf.zeros([self.batch_size, summary_max_len - tf.shape(self.logits)[1], self.vocabulary_size])], axis=1)\n",
    "            else: #testing\n",
    "                tiled_encoder_output = tf.contrib.seq2seq.tile_batch(\n",
    "                    tf.transpose(self.encoder_output, perm=[1, 0, 2]), multiplier=self.beam_width)\n",
    "                tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(self.encoder_state, multiplier=self.beam_width)\n",
    "                tiled_seq_len = tf.contrib.seq2seq.tile_batch(self.X_len, multiplier=self.beam_width)\n",
    "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                    self.num_hidden * 2, tiled_encoder_output, memory_sequence_length=tiled_seq_len, normalize=True)\n",
    "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
    "                                                                   attention_layer_size=self.num_hidden * 2)\n",
    "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size * self.beam_width)\n",
    "                initial_state = initial_state.clone(cell_state=tiled_encoder_final_state)\n",
    "                decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "                    cell=decoder_cell,\n",
    "                    embedding=self.embeddings,\n",
    "                    start_tokens=tf.fill([self.batch_size], tf.constant(2)),\n",
    "                    end_token=tf.constant(3),\n",
    "                    initial_state=initial_state,\n",
    "                    beam_width=self.beam_width,\n",
    "                    output_layer=self.projection_layer\n",
    "                )\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    decoder, output_time_major=True, maximum_iterations=summary_max_len, scope=decoder_scope)\n",
    "                self.prediction = tf.transpose(outputs.predicted_ids, perm=[1, 2, 0])\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            if not forward_only: #training\n",
    "                crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    logits=self.logits_reshape, labels=self.decoder_target)\n",
    "                weights = tf.sequence_mask(self.decoder_len, summary_max_len, dtype=tf.float32)\n",
    "                self.loss = tf.reduce_sum(crossent * weights / tf.to_float(self.batch_size))\n",
    "                # TODO: Regularization\n",
    "                params = tf.trainable_variables()\n",
    "                gradients = tf.gradients(self.loss, params)\n",
    "                clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "                optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "                self.update = optimizer.apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "WARNING:tensorflow:From /home/rmodi/anaconda3/envs/nlp-project/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-5-7c823a822e05>:72: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/rmodi/anaconda3/envs/nlp-project/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/rnn.py:233: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/rmodi/anaconda3/envs/nlp-project/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/rmodi/anaconda3/envs/nlp-project/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/rmodi/anaconda3/envs/nlp-project/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-5-7c823a822e05>:134: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration starts.\n",
      "Number of batches per epoch : 3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [05:33,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: loss = 52.490299224853516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1961it [10:58,  3.10it/s]"
     ]
    }
   ],
   "source": [
    "print(\"Starting Training\")\n",
    "                \n",
    "from tqdm import tqdm\n",
    "import time\n",
    "start = time.perf_counter()\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class args:\n",
    "    pass\n",
    "  \n",
    "args.num_hidden=150\n",
    "args.num_layers=2\n",
    "args.beam_width=10\n",
    "args.glove=\"store_true\"\n",
    "args.embedding_size=320\n",
    "\n",
    "args.learning_rate=1e-3\n",
    "args.batch_size=64\n",
    "args.num_epochs=10\n",
    "args.keep_prob = 0.8\n",
    "\n",
    "args.toy=False #\"store_true\"\n",
    "\n",
    "args.with_model=\"store_true\"\n",
    "\n",
    "if not os.path.exists(default_path + \"saved_model_textrank\"):\n",
    "    os.mkdir(default_path + \"saved_model_textrank\")\n",
    "else:\n",
    "    if args.with_model:\n",
    "        old_model_checkpoint_path = open(default_path + 'saved_model_textrank/checkpoint', 'r')\n",
    "#         old_model_checkpoint_path = \"\".join([default_path + \"saved_model_2/\",old_model_checkpoint_path.read().splitlines()[0].split('\"')[1] ])\n",
    "        old_model_checkpoint_path = old_model_checkpoint_path.read().splitlines()[0].split('\"')[1]\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model = Model(reversed_dict, article_max_len, summary_max_len, args)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    " \n",
    "    batches = batch_iter(train_x, train_y, args.batch_size, args.num_epochs)\n",
    "    num_batches_per_epoch = (len(train_x) - 1) // args.batch_size + 1\n",
    "\n",
    "    print(\"\\nIteration starts.\")\n",
    "    print(\"Number of batches per epoch :\", num_batches_per_epoch)\n",
    "    total_batches_done = 0\n",
    "    \n",
    "    \n",
    "    for batch_x, batch_y, sentence_ids in tqdm(batches):\n",
    "        batch_x_len = list(map(lambda x: len([y for y in x if y != 0]), batch_x))\n",
    "        batch_decoder_input = list(map(lambda x: [word_dict[\"<s>\"]] + list(x), batch_y))\n",
    "        batch_decoder_len = list(map(lambda x: len([y for y in x if y != 0]), batch_decoder_input))\n",
    "        batch_decoder_output = list(map(lambda x: list(x) + [word_dict[\"</s>\"]], batch_y))\n",
    "\n",
    "        batch_decoder_input = list(\n",
    "            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_input))\n",
    "        batch_decoder_output = list(\n",
    "            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_output))\n",
    "        \n",
    "        total_batches_done += 1\n",
    "        \n",
    "        train_feed_dict = {\n",
    "            model.batch_size: len(batch_x),\n",
    "            model.X: batch_x,\n",
    "            model.X_len: batch_x_len,\n",
    "            model.decoder_input: batch_decoder_input,\n",
    "            model.decoder_len: batch_decoder_len,\n",
    "            model.decoder_target: batch_decoder_output,\n",
    "            model.sentence_ids: sentence_ids\n",
    "        }\n",
    "\n",
    "        _, step, loss = sess.run([model.update, model.global_step, model.loss], feed_dict=train_feed_dict)\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            print(\"step {0}: loss = {1}\".format(step, loss))\n",
    "\n",
    "        if step % num_batches_per_epoch == 0:\n",
    "            hours, rem = divmod(time.perf_counter() - start, 3600)\n",
    "            minutes, seconds = divmod(rem, 60)\n",
    "            saver.save(sess, default_path + \"saved_model_textrank/model.ckpt\", global_step=step)\n",
    "            print(\" Epoch {0}: Model is saved.\".format(step // num_batches_per_epoch),\n",
    "            \"Elapsed: {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds) , \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building validation sentence textrank matrix\")\n",
    "\n",
    "sentence_textrank_valid_mat = build_train_sentence_textrank_mat(\"valid\", word_dict, article_max_len, summary_max_len, True)\n",
    "\n",
    "print(\"Max value in sentence_textrank_valid_mat: \" + str(np.max(sentence_textrank_valid_mat)) + \" Shape: \" + str(sentence_textrank_valid_mat.shape))\n",
    "\n",
    "\n",
    "if sentence_textrank_valid_mat.shape[0] < textrank_train_len:\n",
    "    padding_rows = np.zeros((textrank_train_len - sentence_textrank_valid_mat.shape[0], sentence_textrank_valid_mat.shape[1]))\n",
    "    sentence_textrank_valid_mat = np.concatenate([sentence_textrank_valid_mat, padding_rows])\n",
    "\n",
    "print(\"Shape of sentence_textrank_valid_mat after padding: \" + str(sentence_textrank_valid_mat.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dictionary...\n",
      "Loading validation dataset...\n",
      "Loading article and reference...\n",
      "Loading saved model...\n",
      "INFO:tensorflow:Restoring parameters from /media/mount/Users/Ruchit Modi/Documents/CSE538/project/Model2/dataset/saved_model_baseline/model.ckpt-6250\n",
      "Writing summaries to 'textrank_results.txt'...\n",
      "Summaries have been generated\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "class args:\n",
    "    pass\n",
    "  \n",
    "args.num_hidden=150\n",
    "args.num_layers=2\n",
    "args.beam_width=10\n",
    "args.glove=\"store_true\"\n",
    "args.embedding_size=320\n",
    "\n",
    "args.learning_rate=1e-3\n",
    "args.batch_size=64\n",
    "args.num_epochs=10\n",
    "args.keep_prob = 0.8\n",
    "\n",
    "args.toy=True\n",
    "\n",
    "args.with_model=\"store_true\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Loading dictionary...\")\n",
    "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"valid\", args.toy)\n",
    "print(\"Loading validation dataset...\")\n",
    "valid_x = build_dataset(\"valid\", word_dict, article_max_len, summary_max_len, args.toy)\n",
    "valid_x_len = [len([y for y in x if y != 0]) for x in valid_x]\n",
    "print(\"Loading article and reference...\")\n",
    "article = get_text_list(valid_article_path, args.toy)\n",
    "reference = get_text_list(valid_title_path, args.toy)\n",
    "\n",
    "\n",
    "f = open(default_path + \"textrank_results.txt\", \"w\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"Loading saved model...\")\n",
    "    model = Model(reversed_dict, article_max_len, summary_max_len, args, forward_only=True)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    ckpt = tf.train.get_checkpoint_state(default_path + \"saved_model_baseline_2epoch_200000/\")\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    batches = batch_iter(valid_x, [0] * len(valid_x), args.batch_size, 1)\n",
    "\n",
    "    print(\"Writing summaries to 'textrank_results.txt'...\")\n",
    "    i = 0\n",
    "    for batch_x, _, sentence_ids in batches:\n",
    "        batch_x_len = [len([y for y in x if y != 0]) for x in batch_x]\n",
    "\n",
    "        valid_feed_dict = {\n",
    "            model.batch_size: len(batch_x),\n",
    "            model.X: batch_x,\n",
    "            model.X_len: batch_x_len,\n",
    "            model.sentence_ids: sentence_ids\n",
    "        }\n",
    "\n",
    "        prediction = sess.run(model.prediction, feed_dict=valid_feed_dict)\n",
    "        prediction_output = [[reversed_dict[y] for y in x] for x in prediction[:, 0, :]]\n",
    "        summary_array = []\n",
    "        j = 0\n",
    "        for line in prediction_output:\n",
    "            summary = list()\n",
    "            for word in line:\n",
    "                if word == \"</s>\":\n",
    "                    break\n",
    "                if word not in summary:\n",
    "                    summary.append(word)\n",
    "            summary_array.append(\" \".join(summary))\n",
    "            write_line = '==============>>>>>>>>'.join([article[i+j], \" \".join(summary)]) + '\\n'\n",
    "            separator = \"===========================================\"\n",
    "            write_line = article[i+j] + \"\\n\" + separator + \"\\n\" + reference[i+j] + \"\\n\" + separator + \"\\n\" +  \" \".join(summary) + \"\\n\"\n",
    "            write_line = write_line + \"++++++++++++++++++++++++++++++++++++++++++++++++++++\" + \"\\n\" + \"\\n\"\n",
    "            f.write(write_line)\n",
    "            j += 1\n",
    "            #print(\" \".join(summary), file=f)\n",
    "        i += len(batch_x)\n",
    "\n",
    "    print('Summaries have been generated')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_array = []\n",
    "for line in prediction_output:\n",
    "    summary = list()\n",
    "    for word in line:\n",
    "        if word == \"</s>\":\n",
    "            break\n",
    "        if word not in summary:\n",
    "            summary.append(word)\n",
    "    summary_array.append(\" \".join(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b.egyptian=(amod)=>guards\n",
      "a.guards=(nsubj)=>killed\n",
      "<BasicElement: guards-[nsubj]->kill>\n",
      "b.egyptian=(amod)=>guards\n",
      "a.guards=(nsubj)=>killed\n",
      "<BasicElement: guards-[nsubj]->kill>\n",
      "ROUGE-1: 0.8000000000000002\n",
      "ROUGE-2: 0.25\n",
      "ROUGE-L: 0.6\n",
      "ROUGE-BE: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8000000000000002, 0.25, 0.6, 1.0, 2.8634401465295505)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://github.com/chakki-works/sumeval\n",
    "#https://github.com/Tian312/awesome-text-summarization\n",
    "\n",
    "from sumeval.metrics.rouge import RougeCalculator\n",
    "from sumeval.metrics.bleu import BLEUCalculator\n",
    "\n",
    "def eval_rouges(refrence_summary,model_summary):\n",
    "    rouge = RougeCalculator(stopwords=True, lang=\"en\")\n",
    "\n",
    "    rouge_1 = rouge.rouge_n(\n",
    "                summary=model_summary,\n",
    "                references=refrence_summary,\n",
    "                n=1)\n",
    "\n",
    "    rouge_2 = rouge.rouge_n(\n",
    "                summary=model_summary,\n",
    "                references=[refrence_summary],\n",
    "                n=2)\n",
    "    \n",
    "    rouge_l = rouge.rouge_l(\n",
    "                summary=model_summary,\n",
    "                references=[refrence_summary])\n",
    "    \n",
    "    # You need spaCy to calculate ROUGE-BE\n",
    "    \n",
    "    rouge_be = rouge.rouge_be(\n",
    "                summary=model_summary,\n",
    "                references=[refrence_summary])\n",
    "\n",
    "    bleu = BLEUCalculator()\n",
    "    bleu_score = bleu.bleu( summary=model_summary,\n",
    "                        references=[refrence_summary])\n",
    "\n",
    "    print(\"ROUGE-1: {}, ROUGE-2: {}, ROUGE-L: {}, ROUGE-BE: {}\".format(\n",
    "        rouge_1, rouge_2, rouge_l, rouge_be\n",
    "    ).replace(\", \", \"\\n\"))\n",
    "    \n",
    "    return rouge_1, rouge_2,rouge_l,rouge_be,bleu_score\n",
    "\n",
    "refrence_summary = \"two egyptian guards killed on border with gaza\"\n",
    "model_summary = \"two egyptian border guards killed in clashes\"\n",
    "eval_rouges(refrence_summary,model_summary)\n",
    "#rouge_1, rouge_2,rouge_l,rouge_be = eval_rouges( \"tokyo shares close up #.## percent\",  \"tokyo stocks close up # percent to fresh record high\")\n",
    "\n",
    "#print(\"ROUGE-1: {}, ROUGE-2: {}, ROUGE-L: {}, ROUGE-BE: {}\".format(     rouge_1, rouge_2, rouge_l, rouge_be).replace(\", \", \"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b.skating=(compound)=>championships\n",
      "a.injury=(nsubj)=>leaves\n",
      "<BasicElement: injury-[nsubj]->leave>\n",
      "a.hopes=(dobj)=>leaves\n",
      "<BasicElement: hopes-[dobj]->leave>\n",
      "a.leaders=(nsubj)=>lash\n",
      "<BasicElement: leaders-[nsubj]->lash>\n",
      "b.illegal=(amod)=>immigrants\n",
      "b.tough=(amod)=>law\n",
      "a.sales=(nsubj)=>fall\n",
      "<BasicElement: sales-[nsubj]->fall>\n",
      "a.percent=(npadvmod)=>fall\n",
      "<BasicElement: percent-[npadvmod]->fall>\n",
      "a.thousands=(nsubj)=>celebrate\n",
      "<BasicElement: thousands-[nsubj]->celebrate>\n",
      "b.liberian=(amod)=>president\n",
      "a.president=(dobj)=>attend\n",
      "<BasicElement: president-[dobj]->attend>\n",
      "b.attend=(advcl)=>rice\n",
      "a.inauguration=(dobj)=>attend\n",
      "<BasicElement: inauguration-[dobj]->attend>\n",
      "b.top=(amod)=>lobbyist\n",
      "b.republican=(amod)=>lobbyist\n",
      "a.lobbyist=(nsubj)=>pleads\n",
      "<BasicElement: lobbyist-[nsubj]->plead>\n",
      "a.speaker=(nsubj)=>agrees\n",
      "<BasicElement: speaker-[nsubj]->agree>\n",
      "a.workers=(nsubj)=>strike\n",
      "<BasicElement: workers-[nsubj]->strike>\n",
      "b.portuguese=(amod)=>workers\n",
      "a.strike=(nsubj)=>ground\n",
      "<BasicElement: strike-[nsubj]->ground>\n",
      "a.flights=(dobj)=>ground\n",
      "<BasicElement: flights-[dobj]->ground>\n",
      "a.signs=(compound)=>deal\n",
      "<BasicElement: signs-[compound]->deal>\n",
      "a.support=(dobj)=>shores\n",
      "<BasicElement: support-[dobj]->shore>\n",
      "a.nationals=(dobj)=>advises\n",
      "<BasicElement: nationals-[dobj]->advise>\n",
      "a.travel=(dobj)=>avoiding\n",
      "<BasicElement: travel-[dobj]->avoid>\n",
      "b.seen=(acl)=>sales\n",
      "b.weaker=(amod)=>sales\n",
      "a.sales=(dobj)=>sees\n",
      "<BasicElement: sales-[dobj]->see>\n",
      "a.release=(dobj)=>welcomes\n",
      "<BasicElement: release-[dobj]->welcome>\n",
      "b.chinese=(amod)=>journalist\n",
      "b.chinese=(amod)=>journalist\n",
      "a.journalist=(dobj)=>welcomes\n",
      "<BasicElement: journalist-[dobj]->welcome>\n",
      "a.bankers=(nsubj)=>admit\n",
      "<BasicElement: bankers-[nsubj]->admit>\n",
      "a.theft=(dobj)=>admit\n",
      "<BasicElement: theft-[dobj]->admit>\n",
      "a.hospital=(xcomp)=>admitted\n",
      "<BasicElement: hospital-[xcomp]->admit>\n",
      "b.hybrid=(amod)=>sales\n",
      "a.sales=(nsubj)=>expect\n",
      "<BasicElement: sales-[nsubj]->expect>\n",
      "b.fierce=(amod)=>assault\n",
      "a.assault=(dobj)=>admits\n",
      "<BasicElement: assault-[dobj]->admit>\n",
      "a.battle=(dobj)=>admits\n",
      "<BasicElement: battle-[dobj]->admit>\n",
      "a.troops=(dobj)=>rejects\n",
      "<BasicElement: troops-[dobj]->reject>\n",
      "a.soldiers=(nsubj)=>act\n",
      "<BasicElement: soldiers-[nsubj]->act>\n",
      "a.civilians=(dobj)=>protect\n",
      "<BasicElement: civilians-[dobj]->protect>\n",
      "b.special=(amod)=>envoy\n",
      "b.special=(amod)=>envoy\n",
      "a.envoy=(nsubj)=>quits\n",
      "<BasicElement: envoy-[nsubj]->quit>\n",
      "b.korean=(amod)=>talks\n",
      "b.nuclear=(amod)=>talks\n",
      "b.positive=(amod)=>test\n",
      "a.envoy=(nsubj)=>urges\n",
      "<BasicElement: envoy-[nsubj]->urge>\n",
      "a.support=(dobj)=>urges\n",
      "<BasicElement: support-[dobj]->urge>\n",
      "b.stronger=(amod)=>support\n",
      "b.international=(amod)=>support\n",
      "a.support=(dobj)=>urges\n",
      "<BasicElement: support-[dobj]->urge>\n",
      "a.others=(dobj)=>backs\n",
      "<BasicElement: others-[dobj]->back>\n",
      "b.egyptian=(amod)=>guards\n",
      "a.guards=(nsubj)=>killed\n",
      "<BasicElement: guards-[nsubj]->kill>\n",
      "b.egyptian=(amod)=>guards\n",
      "a.guards=(nsubj)=>killed\n",
      "<BasicElement: guards-[nsubj]->kill>\n",
      "a.conservatives=(nsubj)=>gain\n",
      "<BasicElement: conservatives-[nsubj]->gain>\n",
      "a.momentum=(dobj)=>gain\n",
      "<BasicElement: momentum-[dobj]->gain>\n",
      "b.ruling=(amod)=>liberals\n",
      "a.unk=(nmod)=>resigns\n",
      "<BasicElement: unk-[nmod]->resign>\n",
      "a.bills=(nsubj)=>shake\n",
      "<BasicElement: bills-[nsubj]->shake>\n",
      "b.front=(amod)=>office\n",
      "a.office=(dobj)=>shake\n",
      "<BasicElement: office-[dobj]->shake>\n",
      "a.dollar=(dobj)=>sends\n",
      "<BasicElement: dollar-[dobj]->send>\n",
      "a.actor=(nsubj)=>wins\n",
      "<BasicElement: actor-[nsubj]->win>\n",
      "a.nominations=(dobj)=>wins\n",
      "<BasicElement: nominations-[dobj]->win>\n",
      "b.top=(amod)=>groups\n",
      "a.groups=(nsubj)=>pick\n",
      "<BasicElement: groups-[nsubj]->pick>\n",
      "a.nominees=(dobj)=>pick\n",
      "<BasicElement: nominees-[dobj]->pick>\n",
      "a.oscars=(compound)=>loom\n",
      "<BasicElement: oscars-[compound]->loom>\n",
      "a.policy=(dobj)=>praises\n",
      "<BasicElement: policy-[dobj]->praise>\n",
      "a.concern=(dobj)=>shares\n",
      "<BasicElement: concern-[dobj]->share>\n",
      "a.court=(nsubj)=>seeks\n",
      "<BasicElement: court-[nsubj]->seek>\n",
      "a.order=(dobj)=>seeks\n",
      "<BasicElement: order-[dobj]->seek>\n",
      "b.stop=(acl)=>order\n",
      "a.divorce=(dobj)=>stop\n",
      "<BasicElement: divorce-[dobj]->stop>\n",
      "a.man=(nsubj)=>seeks\n",
      "<BasicElement: man-[nsubj]->seek>\n",
      "a.order=(dobj)=>seeks\n",
      "<BasicElement: order-[dobj]->seek>\n",
      "b.ministerial=(amod)=>session\n",
      "a.session=(dobj)=>hold\n",
      "<BasicElement: session-[dobj]->hold>\n",
      "b.former=(amod)=>pm\n",
      "b.israeli=(amod)=>leaders\n",
      "a.leaders=(nsubj)=>unite\n",
      "<BasicElement: leaders-[nsubj]->unite>\n",
      "a.operation=(dobj)=>undergoes\n",
      "<BasicElement: operation-[dobj]->undergo>\n",
      "b.human=(amod)=>rights\n",
      "a.victims=(nsubjpass)=>expelled\n",
      "<BasicElement: victims-[nsubjpass]->expel>\n",
      "b.human=(amod)=>trafficking\n",
      "a.sabres=(nsubj)=>sign\n",
      "<BasicElement: sabres-[nsubj]->sign>\n",
      "b.new=(amod)=>meeting\n",
      "a.meeting=(dobj)=>chair\n",
      "<BasicElement: meeting-[dobj]->chair>\n",
      "a.years=(dobj)=>wins\n",
      "<BasicElement: years-[dobj]->win>\n",
      "a.unk=(dobj)=>ends\n",
      "<BasicElement: unk-[dobj]->end>\n",
      "a.confidence=(nsubj)=>tops\n",
      "<BasicElement: confidence-[nsubj]->top>\n",
      "a.benchmark=(advmod)=>tops\n",
      "<BasicElement: benchmark-[advmod]->top>\n",
      "a.gold=(nsubj)=>opens\n",
      "<BasicElement: gold-[nsubj]->open>\n",
      "a.gold=(nsubj)=>opens\n",
      "<BasicElement: gold-[nsubj]->open>\n",
      "a.percent=(npadvmod)=>higher\n",
      "<BasicElement: percent-[npadvmod]->high>\n",
      "a.shares=(nsubj)=>rise\n",
      "<BasicElement: shares-[nsubj]->rise>\n",
      "a.percent=(dobj)=>rise\n",
      "<BasicElement: percent-[dobj]->rise>\n",
      "a.officer=(nsubj)=>seeks\n",
      "<BasicElement: officer-[nsubj]->seek>\n",
      "b.british=(amod)=>police\n",
      "a.police=(nsubj)=>seek\n",
      "<BasicElement: police-[nsubj]->seek>\n",
      "b.key=(amod)=>facts\n",
      "b.hemorrhagic=(amod)=>stroke\n",
      "a.shares=(nsubj)=>open\n",
      "<BasicElement: shares-[nsubj]->open>\n",
      "a.percent=(npadvmod)=>higher\n",
      "<BasicElement: percent-[npadvmod]->high>\n",
      "a.shares=(nsubj)=>open\n",
      "<BasicElement: shares-[nsubj]->open>\n",
      "a.ease=(advcl)=>open\n",
      "<BasicElement: ease-[advcl]->open>\n",
      "b.korean=(amod)=>doubles\n"
     ]
    }
   ],
   "source": [
    "#https://pymotw.com/2/xml/etree/ElementTree/create.html\n",
    "\n",
    "bleu_arr = []\n",
    "rouge_1_arr  = []\n",
    "rouge_2_arr  = []\n",
    "rouge_L_arr  = []\n",
    "rouge_be_arr = []\n",
    "\n",
    "from xml.etree import ElementTree\n",
    "from xml.dom import minidom\n",
    "from functools import reduce\n",
    "\n",
    "def prettify(elem):\n",
    "    \"\"\"Return a pretty-printed XML string for the Element.\n",
    "    \"\"\"\n",
    "    rough_string = ElementTree.tostring(elem, 'utf-8')\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    return reparsed.toprettyxml(indent=\"  \")\n",
    "  \n",
    "from xml.etree.ElementTree import Element, SubElement, Comment\n",
    "\n",
    "top = Element('TextRank Evaluation')\n",
    "\n",
    "comment = Comment('Generated by Amr Zaki')\n",
    "top.append(comment)\n",
    "\n",
    "i=0\n",
    "for summ in summary_array:\n",
    "    example = SubElement(top, 'example')\n",
    "    article_element   = SubElement(example, 'article')\n",
    "    article_element.text = article[i]\n",
    "\n",
    "    reference_element = SubElement(example, 'reference')\n",
    "    reference_element.text = reference[i]\n",
    "\n",
    "    summary_element   = SubElement(example, 'summary')\n",
    "    summary_element.text = summ\n",
    "\n",
    "    rouge_1, rouge_2,rouge_L,rouge_be,bleu_score = eval_rouges(reference[i],summ )\n",
    "\n",
    "    eval_element = SubElement(example, 'eval')\n",
    "    bleu_score_element = SubElement(eval_element,'BLEU', {'score':str(bleu_score)})\n",
    "    ROUGE_1_element  = SubElement(eval_element, 'ROUGE_1' , {'score':str(rouge_1)})\n",
    "    ROUGE_2_element  = SubElement(eval_element, 'ROUGE_2' , {'score':str(rouge_2)})\n",
    "    ROUGE_L_element  = SubElement(eval_element, 'ROUGE_l' , {'score':str(rouge_L)})\n",
    "    ROUGE_be_element  = SubElement(eval_element,'ROUGE_be', {'score':str(rouge_be)})\n",
    "\n",
    "    bleu_arr.append(bleu_score) \n",
    "    rouge_1_arr.append(rouge_1) \n",
    "    rouge_2_arr.append(rouge_2) \n",
    "    rouge_L_arr.append(rouge_L) \n",
    "    rouge_be_arr.append(rouge_be) \n",
    "\n",
    "    i+=1\n",
    "\n",
    "top.set('bleu', str(reduce(lambda x, y: x + y,  bleu_arr) / len(bleu_arr)))\n",
    "top.set('rouge_1', str(reduce(lambda x, y: x + y,  rouge_1_arr) / len(rouge_1_arr)))\n",
    "top.set('rouge_2', str(reduce(lambda x, y: x + y,  rouge_2_arr) / len(rouge_2_arr)))\n",
    "top.set('rouge_L', str(reduce(lambda x, y: x + y,  rouge_L_arr) / len(rouge_L_arr)))\n",
    "top.set('rouge_be', str(reduce(lambda x, y: x + y, rouge_be_arr) / len(rouge_be_arr)))\n",
    "\n",
    "results = {}\n",
    "results['bleu'] = {'avg': sum(bleu_arr) / len(bleu_arr), 'min': min(bleu_arr), 'max': max(bleu_arr)}\n",
    "results['rouge_1'] = {'avg': sum(rouge_1_arr) / len(rouge_1_arr), 'min': min(rouge_1_arr), 'max': max(rouge_1_arr)}\n",
    "results['rouge_2'] = {'avg': sum(rouge_2_arr) / len(rouge_2_arr), 'min': min(rouge_2_arr), 'max': max(rouge_2_arr)}\n",
    "results['rouge_L'] = {'avg': sum(rouge_L_arr) / len(rouge_L_arr), 'min': min(rouge_L_arr), 'max': max(rouge_L_arr)}\n",
    "results['rouge_be'] = {'avg': sum(rouge_be_arr) / len(rouge_be_arr), 'min': min(rouge_be_arr), 'max': max(rouge_be_arr)}\n",
    "\n",
    "with open(default_path + 'baseline_evaluation_2.json', 'w') as f:\n",
    "    f.write(json.dumps(results) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text-Summarization-2-features-paper(tf-idf , pos tags)",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Conda: nlp-project",
   "language": "python",
   "name": "nlp-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
